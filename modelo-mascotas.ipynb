{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5b972cc-082b-4f51-aa78-ade71f38cac8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nSMOOTH_TE = 1.0\\nUSE_KFOLD_TE = True\\nN_SPLITS_TE = 5\\nRND = 42\\nTE_ENCODERS = {}\\n\\nHORIZON = 14\\nLEAD_TIME = 7\\nSAFETY = 0.20\\nTOP_K = 10\\n\\nDEFAULT_FLOOR_PCT = 0.00\\ndef enviar_correo(asunto, mensaje):DEFAULT_MARGIN = 0.00'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports, paths y settings\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import tempfile\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV, cross_val_predict\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, OrdinalEncoder\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import lightgbm as lgb\n",
    "import category_encoders as ce\n",
    "from sklearn.linear_model import HuberRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "import gspread\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "from gspread_dataframe import get_as_dataframe, set_with_dataframe\n",
    "\n",
    "from pydrive2.auth import GoogleAuth\n",
    "from pydrive2.drive import GoogleDrive\n",
    "from google.oauth2.service_account import Credentials\n",
    "import json\n",
    "\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from datetime import datetime\n",
    "import traceback\n",
    "\n",
    "fecha_str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "sns.set_style('whitegrid')\n",
    "RND = 42\n",
    "\n",
    "INPUT_DIR = Path('C:/Users/alejo/OneDrive/Escritorio/Modelo mascotas/input/')\n",
    "OUTPUT_DIR = Path('C:/Users/alejo/OneDrive/Escritorio/Modelo mascotas/output/')\n",
    "\n",
    "AUTH_PATH = Path(INPUT_DIR / 'automatizacion-480622-83f05856e13e.json')\n",
    "\n",
    "ARTIFACTS_MODEL_PATH = Path(OUTPUT_DIR / \"artefactos_algoritmo.pkl\")\n",
    "ARTIFACTS_DATA_ENG_PATH = Path(OUTPUT_DIR / \"artefactos_data_engineering.pkl\")\n",
    "\n",
    "PARAMETRIZACION_SHEET = \"parametrizacion\"\n",
    "EJECUCION_SHEET = \"ejecucion\"\n",
    "'''\n",
    "PETTITO_SHEET = \"Master Puppy Information Sheet\"\n",
    "MASTER_WORKSHEET = \"Master Tab\"\n",
    "MIRROR_WORKSHEET = \"Mirror Master Tab\"\n",
    "'''\n",
    "PETTITO_SHEET = \"datos_mascotas\"\n",
    "MASTER_WORKSHEET = \"Hoja1\"\n",
    "MIRROR_WORKSHEET = \"Hoja2\"\n",
    "\n",
    "TRAIN = False\n",
    "NEW_RECORDS = False\n",
    "WORKSHEET_NUEW_RECORDS = \"Hoja5\"\n",
    "\n",
    "'''\n",
    "SMOOTH_TE = 1.0\n",
    "USE_KFOLD_TE = True\n",
    "N_SPLITS_TE = 5\n",
    "RND = 42\n",
    "TE_ENCODERS = {}\n",
    "\n",
    "HORIZON = 14\n",
    "LEAD_TIME = 7\n",
    "SAFETY = 0.20\n",
    "TOP_K = 10\n",
    "\n",
    "DEFAULT_FLOOR_PCT = 0.00\n",
    "def enviar_correo(asunto, mensaje):DEFAULT_MARGIN = 0.00'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01203559-df98-47d5-86df-f123dc814e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enviar_correo(asunto, mensaje):\n",
    "    EMAIL_CONFIG = {\n",
    "        \"smtp_server\": \"smtp.gmail.com\",\n",
    "        \"smtp_port\": 587,\n",
    "        \"sender_email\": \"alejandrovillamil@pettitousa.com\",\n",
    "        \"sender_password\": \"eryw hgrh wmli whxy\",\n",
    "        \"receiver_email\": [\n",
    "            \"alejandrovillamil@pettitousa.com\"\n",
    "            # puedes agregar m√°s correos aqu√≠\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        msg = MIMEMultipart()\n",
    "        msg[\"From\"] = EMAIL_CONFIG[\"sender_email\"]\n",
    "        msg[\"To\"] = \", \".join(EMAIL_CONFIG[\"receiver_email\"])\n",
    "        msg[\"Subject\"] = asunto\n",
    "\n",
    "        msg.attach(MIMEText(mensaje, \"plain\"))\n",
    "\n",
    "        server = smtplib.SMTP(\n",
    "            EMAIL_CONFIG[\"smtp_server\"],\n",
    "            EMAIL_CONFIG[\"smtp_port\"]\n",
    "        )\n",
    "        server.starttls()\n",
    "        server.login(\n",
    "            EMAIL_CONFIG[\"sender_email\"],\n",
    "            EMAIL_CONFIG[\"sender_password\"]\n",
    "        )\n",
    "        server.send_message(msg)\n",
    "        server.quit()\n",
    "\n",
    "        print(\"üìß Correo enviado correctamente\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Error enviando correo:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af1fe05d-a8a2-4f6f-a58c-5ee42d5f22e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Sheet API\n",
    "\n",
    "scope = [\n",
    "    \"https://spreadsheets.google.com/feeds\",\n",
    "    \"https://www.googleapis.com/auth/drive\",\n",
    "]\n",
    "\n",
    "creds_path = os.getenv(\"GOOGLE_CREDENTIALS_PATH\", \"service_account.json\")\n",
    "\n",
    "creds = ServiceAccountCredentials.from_json_keyfile_name(\n",
    "    creds_path, scope\n",
    ")\n",
    "\n",
    "client = gspread.authorize(creds)\n",
    "\n",
    "sheet = client.open(PETTITO_SHEET)\n",
    "sheet_parametrizacion = client.open(PARAMETRIZACION_SHEET)\n",
    "sheet_ejecucion = client.open(EJECUCION_SHEET)\n",
    "\n",
    "# Hojas\n",
    "sheet_master = sheet.worksheet(MASTER_WORKSHEET)\n",
    "worksheet_espejo_actual = sheet.worksheet(MIRROR_WORKSHEET)\n",
    "\n",
    "# Funciones auxiliares\n",
    "def get_sheet_df(worksheet):\n",
    "    \"\"\"Convierte una worksheet a DataFrame limpio\"\"\"\n",
    "    df = get_as_dataframe(worksheet, dtype=str).dropna(how=\"all\")\n",
    "    df = df.loc[:, df.columns.notna()]\n",
    "    df = df.loc[:, df.columns.str.strip() != \"\"]\n",
    "    df.columns = df.columns.str.strip()\n",
    "    return df\n",
    "    \n",
    "def get_price_columns(cols):\n",
    "    \"\"\"Devuelve columnas Price_XX ordenadas por n√∫mero\"\"\"\n",
    "    return sorted(\n",
    "        [c for c in cols if c.startswith(\"Price_\")],\n",
    "        key=lambda x: int(x.split(\"_\")[1])\n",
    "    )\n",
    "\n",
    "def fusionar_historico_precios_updates(df_old, df_new):\n",
    "    \"\"\"\n",
    "    Genera updates y nuevas columnas Price/Date para fusionar df_old -> df_new\n",
    "    \"\"\"\n",
    "    df_new = df_new.copy()\n",
    "    updates = []\n",
    "    new_columns = []\n",
    "\n",
    "    price_cols_old = get_price_columns(df_old.columns)\n",
    "\n",
    "    for _, old_row in df_old.iterrows():\n",
    "        chip = str(old_row[\"Microchip #\"]).strip()\n",
    "        match = df_new[df_new[\"Microchip #\"].astype(str).str.strip() == chip]\n",
    "        if match.empty:\n",
    "            continue\n",
    "\n",
    "        idx_new = match.index[0]\n",
    "\n",
    "        for last_price_col in price_cols_old:\n",
    "            last_date_col = last_price_col.replace(\"Price_\", \"Date_\")\n",
    "            last_price_old = old_row.get(last_price_col)\n",
    "            last_date_old  = old_row.get(last_date_col)\n",
    "\n",
    "            # reemplazar NaN/None por \"\"\n",
    "            if pd.isna(last_price_old) or last_price_old is None:\n",
    "                last_price_old = \"\"\n",
    "            if pd.isna(last_date_old) or last_date_old is None:\n",
    "                last_date_old = \"\"\n",
    "\n",
    "            # agregar columnas nuevas si no existen\n",
    "            if last_price_col not in df_new.columns:\n",
    "                df_new[last_price_col] = \"\"\n",
    "                new_columns.append(last_price_col)\n",
    "            if last_date_col not in df_new.columns:\n",
    "                df_new[last_date_col] = \"\"\n",
    "                new_columns.append(last_date_col)\n",
    "\n",
    "            # preparar updates\n",
    "            updates.append({\"row\": idx_new + 2, \"col_name\": last_price_col, \"value\": last_price_old})\n",
    "            updates.append({\"row\": idx_new + 2, \"col_name\": last_date_col,  \"value\": last_date_old})\n",
    "\n",
    "    return updates, list(set(new_columns))\n",
    "\n",
    "def map_headers_to_cols(worksheet):\n",
    "    \"\"\"Mapear nombre de columna a √≠ndice de columna en Sheet\"\"\"\n",
    "    headers = worksheet.row_values(1)\n",
    "    return {h: i+1 for i, h in enumerate(headers)}\n",
    "\n",
    "def safe_value(val):\n",
    "    \"\"\"Evitar errores de NaN en Google Sheets\"\"\"\n",
    "    if val is None:\n",
    "        return \"\"\n",
    "    if isinstance(val, float) and math.isnan(val):\n",
    "        return \"\"\n",
    "    return val\n",
    "\n",
    "def ordenar_price_date_cols(cols):\n",
    "    \"\"\"Ordena columnas Price_XX y Date_XX intercaladas\"\"\"\n",
    "    price_cols = sorted([c for c in cols if c.startswith(\"Price_\")], key=lambda x: int(x.split(\"_\")[1]))\n",
    "    date_cols  = sorted([c for c in cols if c.startswith(\"Date_\")], key=lambda x: int(x.split(\"_\")[1]))\n",
    "    ordered = []\n",
    "    for p in price_cols:\n",
    "        ordered.append(p)\n",
    "        d = p.replace(\"Price_\", \"Date_\")\n",
    "        if d in date_cols:\n",
    "            ordered.append(d)\n",
    "    other_cols = [c for c in cols if not (c.startswith(\"Price_\") or c.startswith(\"Date_\"))]\n",
    "    return other_cols + ordered\n",
    "\n",
    "# Leer datos\n",
    "df_old = get_sheet_df(worksheet_espejo_actual)\n",
    "\n",
    "worksheet_ejecucion_backup = sheet_ejecucion.add_worksheet(title=f'backup_mirror_{fecha_str}', rows=\"10000\", cols=\"50\")\n",
    "df_old = df_old.fillna(\"\")\n",
    "datos = df_old.values.tolist()\n",
    "encabezados = df_old.columns.tolist()\n",
    "worksheet_ejecucion_backup.update('A1', [encabezados] + datos)\n",
    "\n",
    "worksheet_a_eliminar = sheet.worksheet(MIRROR_WORKSHEET)\n",
    "sheet.del_worksheet(worksheet_a_eliminar)\n",
    "\n",
    "# Duplicar Hoja1 ‚Üí Hoja3 (a la derecha de la master)\n",
    "index_master = sheet_master.index\n",
    "sheet.duplicate_sheet(\n",
    "    source_sheet_id=sheet_master.id,\n",
    "    new_sheet_name=MIRROR_WORKSHEET,\n",
    "    insert_sheet_index=index_master + 1\n",
    ")\n",
    "worksheet_espejo_nueva = sheet.worksheet(MIRROR_WORKSHEET)\n",
    "df_new = get_sheet_df(worksheet_espejo_nueva)\n",
    "\n",
    "# Fusionar precios hist√≥ricos\n",
    "updates, new_columns = fusionar_historico_precios_updates(df_old, df_new)\n",
    "\n",
    "header_map = map_headers_to_cols(worksheet_espejo_nueva)\n",
    "\n",
    "# Crear columnas nuevas si es necesario\n",
    "if new_columns:\n",
    "    worksheet_espejo_nueva.add_cols(len(new_columns))\n",
    "    headers = worksheet_espejo_nueva.row_values(1)\n",
    "    for col in new_columns:\n",
    "        headers.append(col)\n",
    "    # Reordenar columnas Price/Date\n",
    "    headers = ordenar_price_date_cols(headers)\n",
    "    worksheet_espejo_nueva.update('1:1', [headers])\n",
    "    header_map = map_headers_to_cols(worksheet_espejo_nueva)\n",
    "\n",
    "# Crear celdas para actualizar\n",
    "cell_list = [\n",
    "    gspread.Cell(u[\"row\"], header_map[u[\"col_name\"]], safe_value(u[\"value\"]))\n",
    "    for u in updates\n",
    "]\n",
    "\n",
    "# Actualizar por lotes\n",
    "if cell_list:\n",
    "    worksheet_espejo_nueva.update_cells(cell_list)\n",
    "\n",
    "worksheet = sheet.worksheet(MIRROR_WORKSHEET)\n",
    "df_sheet = get_as_dataframe(worksheet, dtype=str).dropna(how=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8347ed5-8aad-4a87-ab69-b817b2a6536d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df: (5124, 26) worksheet_base_piso: (67, 4) est: (391, 6) dic-est: (30, 3) disc-rules: (7, 6)\n",
      "Mapas generados autom√°ticamente:\n",
      " - raza : 71 can√≥nicos\n",
      " - color : 389 can√≥nicos\n",
      " - tienda : 8 can√≥nicos\n",
      " - tamano : 13 can√≥nicos\n",
      " - caracteristica : 16 can√≥nicos\n",
      " - registro : 2 can√≥nicos\n",
      "Completado df: (5120, 27)  predict_df: (355, 27)\n"
     ]
    }
   ],
   "source": [
    "# Data engineering\n",
    "\n",
    "if NEW_RECORDS:\n",
    "    worksheet_new = sheet.worksheet(WORKSHEET_NUEW_RECORDS)\n",
    "    df_sheet_new = get_as_dataframe(worksheet_new, dtype=str).dropna(how=\"all\")\n",
    "\n",
    "    df_new_records = df_sheet_new.copy()\n",
    "    df_new_records.columns = df_new_records.columns.str.strip()\n",
    "    df_new_records = df_new_records[[\n",
    "        \"Microchip#\", \"Breed\", \"Color\", \"Sex  M|F\", \"Breeder\", \"Price\",\n",
    "        \"Arrival Date\", \"Sold date\", \"Location\", \"Idle Days\",\n",
    "        \"Size\", \"Variety\", \"Recibio Registro? SI/NO\"\n",
    "    ]]\n",
    "    df_new_records[\"Status\"] = \"\"\n",
    "    df_new_records.columns = [\n",
    "        \"id\", \"raza\", \"color\", \"genero\", \"criador\", \"precio_venta\",\n",
    "        \"fecha_listado\", \"fecha_venta\", \"tienda\", \"dias_en_tienda\",\n",
    "        \"tamano\", \"caracteristica\", \"registro\", \"estado\"\n",
    "    ]\n",
    "\n",
    "df = df_sheet.copy()\n",
    "# Parametrizacion\n",
    "worksheet_base_piso = sheet_parametrizacion.worksheet('base_piso')\n",
    "df_base_piso = get_sheet_df(worksheet_base_piso)\n",
    "worksheet_estandarizacion = sheet_parametrizacion.worksheet('estandarizacion')\n",
    "df_estandarizacion = get_sheet_df(worksheet_estandarizacion)\n",
    "worksheet_estandarizacion_diccionario = sheet_parametrizacion.worksheet('estandarizacion_diccionario')\n",
    "df_estandarizacion_diccionario = get_sheet_df(worksheet_estandarizacion_diccionario)\n",
    "worksheet_descuento_idle_days = sheet_parametrizacion.worksheet('descuento_idle_days')\n",
    "df_descuento_idle_days = get_as_dataframe(worksheet_descuento_idle_days, dtype=str).dropna(how=\"all\")\n",
    "\n",
    "print(\"df:\", df.shape, \"worksheet_base_piso:\", df_base_piso.shape, \"est:\", df_estandarizacion.shape, \"dic-est:\", df_estandarizacion_diccionario.shape, \"disc-rules:\", df_descuento_idle_days.shape)\n",
    "\n",
    "df.columns = df.columns.str.strip()\n",
    "df = df[[\n",
    "    \"Microchip #\", \"Breed\", \"Color\", \"Sex  M|F\", \"Breeder\", \"Price\",\n",
    "    \"Arrival Date\", \"Sold date\", \"Location\", \"Idle Days\",\n",
    "    \"Size\", \"Variety\", \"Recibio Registro? SI/NO\", \"Status\"\n",
    "]]\n",
    "\n",
    "df.columns = [\n",
    "    \"id\", \"raza\", \"color\", \"genero\", \"criador\", \"precio_venta\",\n",
    "    \"fecha_listado\", \"fecha_venta\", \"tienda\", \"dias_en_tienda\",\n",
    "    \"tamano\", \"caracteristica\", \"registro\", \"estado\"\n",
    "]\n",
    "\n",
    "if NEW_RECORDS:\n",
    "    df_new_records['source'] = 1\n",
    "    df['source'] = 0\n",
    "    df = pd.concat([df, df_new_records], ignore_index=True)\n",
    "\n",
    "df['comentario'] = ''\n",
    "\n",
    "required = ['id','raza','color','genero']\n",
    "df = df.dropna(subset=required, how='any').copy()\n",
    "\n",
    "# Mover valores\n",
    "valores_caracteristica = [\"toy\", \"micro\", \"teacup\", \"mini\"]\n",
    "valores_tamano = [\"moyan\", \"neutered\"]\n",
    "\n",
    "mask_caracteristica = df['caracteristica'].str.lower().isin(valores_caracteristica)\n",
    "df.loc[mask_caracteristica, 'tamano'] = df.loc[mask_caracteristica, 'caracteristica']\n",
    "df.loc[mask_caracteristica, 'caracteristica'] = np.nan\n",
    "\n",
    "mask_tamano = df['tamano'].str.lower().isin(valores_tamano)\n",
    "df.loc[mask_tamano, 'caracteristica'] = df.loc[mask_tamano, 'tamano']\n",
    "df.loc[mask_tamano, 'tamano'] = np.nan\n",
    "\n",
    "df.loc[df['color'].str.contains('tri', case=False, na=False), 'color'] = 'tricolor'\n",
    "\n",
    "CANONICOS = {\n",
    "    'raza': sorted(df_estandarizacion['raza'].dropna().astype(str).str.strip().str.lower().unique()),\n",
    "    'color': sorted(df_estandarizacion['color'].dropna().astype(str).str.strip().str.lower().unique()),\n",
    "    'tienda': sorted(df_estandarizacion['tienda'].dropna().astype(str).str.strip().str.lower().unique()),\n",
    "    'tamano': sorted(df_estandarizacion['tamano'].dropna().astype(str).str.strip().str.lower().unique()),\n",
    "    'caracteristica': sorted(df_estandarizacion['caracteristica'].dropna().astype(str).str.strip().str.lower().unique()),\n",
    "    'registro': sorted(df_estandarizacion['registro'].dropna().astype(str).str.strip().str.lower().unique())\n",
    "}\n",
    "\n",
    "def clean_basic(s):\n",
    "    if pd.isna(s): return \"\"\n",
    "    s = str(s).strip().lower()\n",
    "    s = s.replace('√°','a').replace('√©','e').replace('√≠','i').replace('√≥','o').replace('√∫','u').replace('√±','n')\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    s = re.sub(r'[^a-z0-9\\s&\\-\\+\\(\\)]','',s)\n",
    "    return s\n",
    "\n",
    "df_clean = df.copy()\n",
    "for c in ['raza','color','tienda','tamano','caracteristica','registro']:\n",
    "    df_clean[c] = df_clean[c].astype(str).apply(clean_basic).replace({'nan': np.nan})\n",
    "\n",
    "AUTO_SINONIMOS = { 'raza':{}, 'color':{}, 'tienda':{}, 'tamano':{}, 'caracteristica':{}, 'registro':{} }\n",
    "\n",
    "for campo in ['raza','color','tienda','tamano','caracteristica','registro']:\n",
    "    AUTO_SINONIMOS[campo] = {canon:[canon] for canon in CANONICOS[campo]}\n",
    "    valores = df_clean[campo].dropna().unique().tolist()\n",
    "    for v in valores:\n",
    "        match, score, _ = process.extractOne(v, CANONICOS[campo], scorer=fuzz.token_sort_ratio)\n",
    "        if score >= 85:\n",
    "            # agregar como sin√≥nimo\n",
    "            if v not in AUTO_SINONIMOS[campo][match]:\n",
    "                AUTO_SINONIMOS[campo][match].append(v)\n",
    "        else:\n",
    "            # valor raro ‚Üí no se asigna, quedar√° 'desconocido'\n",
    "            pass\n",
    "\n",
    "print(\"Mapas generados autom√°ticamente:\")\n",
    "\n",
    "for campo, mp in AUTO_SINONIMOS.items():\n",
    "    print(\" -\", campo, \":\", len(mp), \"can√≥nicos\")\n",
    "\n",
    "class SemanticCleaner(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, mapping_dicts, min_freq=0.002, use_fuzzy=True, fuzzy_threshold=85):\n",
    "        \"\"\"\n",
    "        mapping_dicts : dict(col -> {canonico: [sinonimos...]})\n",
    "        min_freq      : puede ser float fijo o dict(col -> float)\n",
    "        \"\"\"\n",
    "        self.mapping_dicts = mapping_dicts\n",
    "        self.min_freq = min_freq\n",
    "        self.use_fuzzy = use_fuzzy\n",
    "        self.fuzzy_threshold = fuzzy_threshold\n",
    "        \n",
    "        # auditor√≠a\n",
    "        self.audit = {k: [] for k in mapping_dicts.keys()}\n",
    "\n",
    "    def _clean(self, s):\n",
    "        if pd.isna(s):\n",
    "            return np.nan\n",
    "        s = str(s).strip().lower()\n",
    "        s = s.replace('√°','a').replace('√©','e').replace('√≠','i').replace('√≥','o').replace('√∫','u').replace('√±','n')\n",
    "        s = re.sub(r'\\s+', ' ', s)\n",
    "        s = re.sub(r'[^a-z0-9\\s&\\-\\+\\(\\)]', '', s)\n",
    "        return s\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.canon_lists = {col: list(self.mapping_dicts[col].keys()) for col in X.columns}\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "    \n",
    "        for col in self.mapping_dicts.keys():\n",
    "            \n",
    "            inv = {}\n",
    "            for canon, sins in self.mapping_dicts[col].items():\n",
    "                for s in sins:\n",
    "                    inv[s] = canon\n",
    "    \n",
    "            def map_one(v):\n",
    "                if pd.isna(v):\n",
    "                    return np.nan\n",
    "    \n",
    "                v_clean = self._clean(v)\n",
    "    \n",
    "                if v_clean in inv:\n",
    "                    self.audit[col].append((v_clean, inv[v_clean], \"exact\"))\n",
    "                    return inv[v_clean]\n",
    "    \n",
    "                if self.use_fuzzy:\n",
    "                    match, score, _ = process.extractOne(\n",
    "                        v_clean, self.canon_lists[col], scorer=fuzz.token_sort_ratio\n",
    "                    )\n",
    "    \n",
    "                    if score >= self.fuzzy_threshold:\n",
    "                        self.audit[col].append((v_clean, match, f\"fuzzy@{score}\"))\n",
    "                        return match\n",
    "                    else:\n",
    "                        self.audit[col].append((v_clean, match, f\"lowconf@{score}\"))\n",
    "                        return np.nan\n",
    "    \n",
    "                return np.nan\n",
    "    \n",
    "            # limpieza + mapeo\n",
    "            X[col] = X[col].apply(map_one)\n",
    "    \n",
    "            if col == \"raza\":\n",
    "                X[\"raza_original\"] = X[col]\n",
    "\n",
    "            if col == \"raza\":\n",
    "                X[\"raza_original\"] = X[col]\n",
    "                X.loc[X[\"raza_original\"] == \"standard goldendoodle\", \"raza_original\"] = \"goldendoodle\"\n",
    "    \n",
    "            # threshold\n",
    "            if isinstance(self.min_freq, dict):\n",
    "                freq_threshold = self.min_freq.get(col, 0.002)\n",
    "            else:\n",
    "                freq_threshold = self.min_freq\n",
    "    \n",
    "            freqs = X[col].value_counts(normalize=True)\n",
    "            low_levels = freqs[freqs < freq_threshold].index.tolist()\n",
    "    \n",
    "            X[col] = X[col].replace(low_levels, 'otros').fillna('desconocido')\n",
    "    \n",
    "        return X\n",
    "\n",
    "for _, row in df_estandarizacion_diccionario.iterrows():\n",
    "    col = row[\"columna\"]\n",
    "    canon = row[\"canonico\"]\n",
    "    sin = row[\"sinonimo\"]\n",
    "    AUTO_SINONIMOS[col][canon].append(sin)\n",
    "\n",
    "df_base_piso['raza'] = df_base_piso['raza'].astype(str).str.strip().str.lower()\n",
    "df['raza'] = df['raza'].astype(str).str.strip().str.lower()\n",
    "cols = ['raza','color','tienda','tamano','caracteristica','registro']\n",
    "\n",
    "sc = SemanticCleaner(\n",
    "    mapping_dicts=AUTO_SINONIMOS,\n",
    "    min_freq={\n",
    "        'raza':0.002,\n",
    "        'color':0.002,\n",
    "        'tienda':0.002,\n",
    "        'tamano':0,\n",
    "        'caracteristica':0,\n",
    "        'registro':0.002\n",
    "    }\n",
    ")\n",
    "sc.fit(df[cols])\n",
    "df = sc.transform(df)\n",
    "\n",
    "df = df.merge(\n",
    "    df_base_piso[['raza','raza_categoria','base','min']].drop_duplicates('raza'),\n",
    "    left_on='raza_original',\n",
    "    right_on='raza',\n",
    "    how='left'\n",
    ")\n",
    "df.drop(columns=['raza_y'], inplace=True)\n",
    "df.rename(columns={'raza_x': 'raza'}, inplace=True)\n",
    "\n",
    "audit_rows = []\n",
    "for col, logs in sc.audit.items():\n",
    "    for ori, mapped, tag in logs:\n",
    "        if tag.startswith(\"lowconf\"):\n",
    "            audit_rows.append({\"col\":col,\"original\":ori,\"mapped_candidate\":mapped,\"tag\":tag})\n",
    "audit_df = pd.DataFrame(audit_rows).drop_duplicates()\n",
    "\n",
    "try:\n",
    "    worksheet_auditoria_estandarizacion = sheet_parametrizacion.worksheet('auditoria_estandarizacion')\n",
    "    sheet_parametrizacion.del_worksheet(worksheet_auditoria_estandarizacion)\n",
    "except gspread.exceptions.WorksheetNotFound:\n",
    "    print(f\"La hoja 'auditoria_estandarizacion' no existe. Se crear√° una nueva.\")\n",
    "\n",
    "worksheet_auditoria_estandarizacion = sheet_parametrizacion.add_worksheet(title='auditoria_estandarizacion', rows=\"200\", cols=\"20\")\n",
    "\n",
    "audit_df = audit_df.fillna(\"\")\n",
    "datos = audit_df.values.tolist()\n",
    "encabezados = audit_df.columns.tolist()\n",
    "\n",
    "worksheet_auditoria_estandarizacion.update('A1', [encabezados] + datos)\n",
    "\n",
    "mask_missing = df['raza_categoria'].isna()\n",
    "\n",
    "df['fecha_listado'] = pd.to_datetime(df['fecha_listado'], errors='coerce')\n",
    "df['fecha_venta']   = pd.to_datetime(df['fecha_venta'],   errors='coerce')\n",
    "df['vendido_flag'] = df['fecha_venta'].notna().astype(int)\n",
    "df['precio_venta'] = pd.to_numeric(df['precio_venta'].str.replace(r'[$,]', '', regex=True),errors='coerce')\n",
    "\n",
    "ref_date = pd.Timestamp.now()\n",
    "#ref_date = pd.Timestamp('2025-12-30')\n",
    "df['dias_en_tienda'] = (df['fecha_venta'].fillna(ref_date) - df['fecha_listado']).dt.days.clip(lower=0)\n",
    "\n",
    "agg_raza = df[df['vendido_flag']==1].groupby('raza').agg(\n",
    "    med_dias=('dias_en_tienda','median'),\n",
    "    med_precio=('precio_venta','median'),\n",
    "    n_raza=('id','count')\n",
    ").reset_index()\n",
    "\n",
    "agg_raza['rotacion'] = agg_raza['n_raza'] / agg_raza['med_dias'].replace(0,np.nan)\n",
    "agg_raza['rotacion'] = agg_raza['rotacion'].fillna(0)\n",
    "\n",
    "q_d = agg_raza['med_dias'].quantile([0.1,0.3,0.5,0.7,0.9]).values\n",
    "q_p = agg_raza['med_precio'].quantile([0.1,0.3,0.5,0.7,0.9]).values\n",
    "q_r = agg_raza['rotacion'].quantile([0.1,0.3,0.5,0.7,0.9]).values\n",
    "q_n = agg_raza['n_raza'].quantile([0.1,0.3,0.5,0.7,0.9]).values\n",
    "\n",
    "def map_cat(d,p,r,n):\n",
    "    try:\n",
    "        if d<=q_d[0] and p>=q_p[4] and r>=q_r[4] and n>=q_n[4]: return 'A+'\n",
    "        if d<=q_d[1] and p>=q_p[3] and r>=q_r[3] and n>=q_n[3]: return 'A'\n",
    "        if d<=q_d[2] and p>=q_p[2] and r>=q_r[2] and n>=q_n[2]: return 'B'\n",
    "        if d<=q_d[3] and p<=q_p[1] and r>=q_r[1] and n>=q_n[1]: return 'C'\n",
    "        if d<=q_d[4] and p<=q_p[0] and r>=q_r[0] and n>=q_n[0]: return 'D'\n",
    "        if d> q_d[4] and p<=q_p[0] and r< q_r[0] and n< q_n[0]: return 'E'\n",
    "    except:\n",
    "        pass\n",
    "    return 'F'\n",
    "\n",
    "agg_raza['raza_categoria_advanced'] = agg_raza.apply(\n",
    "    lambda row: map_cat(row['med_dias'], row['med_precio'], row['rotacion'], row['n_raza']),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "for c in ['med_precio', 'n_raza', 'rotacion']:\n",
    "    if c not in df.columns:\n",
    "        df[c] = np.nan\n",
    "\n",
    "if mask_missing.any():\n",
    "    df_missing = df.loc[mask_missing,['raza']].drop_duplicates().merge(\n",
    "        agg_raza[['raza','raza_categoria_advanced','med_precio','n_raza','rotacion']],\n",
    "        on='raza',\n",
    "        how='left'\n",
    "    )\n",
    "    for _, r in df_missing.iterrows():\n",
    "        cond = (df['raza']==r['raza']) & (df['raza_categoria'].isna())\n",
    "        df.loc[cond,'raza_categoria'] = r['raza_categoria_advanced']\n",
    "        df.loc[cond,'med_precio']     = r['med_precio']\n",
    "        df.loc[cond,'n_raza']         = r['n_raza']\n",
    "        df.loc[cond,'rotacion']       = r['rotacion']\n",
    "        df.loc[cond,'comentario']     = \"No se encontro raza en archivo 'raza_base_piso'\"\n",
    "\n",
    "df['raza_categoria'] = df['raza_categoria'].fillna('F')\n",
    "df['med_precio']     = df['med_precio'].fillna(df['med_precio'].median())\n",
    "df['n_raza']         = df['n_raza'].fillna(0)\n",
    "df['rotacion']       = df['rotacion'].fillna(0)\n",
    "\n",
    "agg_t = df.groupby('tienda').agg(\n",
    "    precio_mean_tienda=('precio_venta','mean'),\n",
    "    ventas_total=('vendido_flag','sum'),\n",
    "    total_listados=('id','count')\n",
    ").reset_index()\n",
    "\n",
    "agg_t['prob_vender_tienda'] = agg_t['ventas_total'] / agg_t['total_listados']\n",
    "df = df.merge(agg_t[['tienda','precio_mean_tienda','prob_vender_tienda']], on='tienda', how='left')\n",
    "\n",
    "df['raza_x_tienda'] = df['raza'] + '___' + df['tienda']\n",
    "df['raza_x_color']  = df['raza'] + '___' + df['color']\n",
    "\n",
    "for col in ['raza_x_tienda','raza_x_color']:\n",
    "    freq = df[col].value_counts(normalize=True)\n",
    "    low = freq[freq < 0.001].index\n",
    "    df[col] = df[col].replace(low, 'otros')\n",
    "\n",
    "df['estado'] = df['estado'].astype(str).str.strip().str.lower()\n",
    "\n",
    "if NEW_RECORDS:\n",
    "    predict_df = df[(df['source'] == 1)].copy().reset_index(drop=True)\n",
    "else:\n",
    "    predict_df = df[(df['vendido_flag'] == 0) & (df['estado'] != 'inactive')].copy().reset_index(drop=True)\n",
    "\n",
    "print(\"Completado df:\", df.shape, \" predict_df:\", predict_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e82a2328-2c83-4552-9585-cf9e6b72678d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'spreadsheetId': '1hXFWBWN0YB2ScAPa8zctBdpiZHC93PT8zd5kisXAVLo',\n",
       " 'updatedRange': 'inventario_tiendas!A1:Q77',\n",
       " 'updatedRows': 77,\n",
       " 'updatedColumns': 17,\n",
       " 'updatedCells': 1309}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inventario tiendas\n",
    "\n",
    "df_filtered = df[(df['tienda'] != 'otros') & (df['raza'] != 'otros') & (df['color'] != 'otros')]\n",
    "\n",
    "combo = df_filtered.groupby(['tienda','raza','color']).agg(\n",
    "    n_listados=('id','count'),\n",
    "    n_vendidos=('vendido_flag','sum'),\n",
    "    precio_promedio=('precio_venta','mean'),\n",
    "    dias_medianos=('dias_en_tienda','median'),\n",
    "    prob_mean=('vendido_flag','mean')\n",
    ").reset_index()\n",
    "\n",
    "combo['dias_medianos'] = combo['dias_medianos'].replace(0, 1)\n",
    "combo['rotacion'] = combo['n_vendidos'] / combo['dias_medianos']\n",
    "combo['ventas_esperadas_14d'] = combo['prob_mean'] * combo['n_listados']\n",
    "\n",
    "def normalize(series):\n",
    "    return (series - series.min()) / (series.max() - series.min() + 1e-9)\n",
    "\n",
    "combo['prob_norm'] = normalize(combo['prob_mean'])\n",
    "combo['rot_norm'] = normalize(combo['rotacion'])\n",
    "combo['precio_norm'] = normalize(combo['precio_promedio'])\n",
    "\n",
    "combo['score'] = (\n",
    "    0.35 * combo['prob_norm'] +\n",
    "    0.30 * combo['rot_norm'] +\n",
    "    0.20 * combo['precio_norm'] +\n",
    "    0.10 * np.log1p(combo['n_listados']) +\n",
    "    0.05 * (1 / (1 + combo['dias_medianos']))\n",
    ")\n",
    "\n",
    "HORIZON = 14\n",
    "LEAD_TIME = 7\n",
    "SAFETY = 0.20\n",
    "TOP_K = 10\n",
    "\n",
    "combo['inventario_base'] = combo['ventas_esperadas_14d'] * (LEAD_TIME / HORIZON)\n",
    "combo['safety_stock'] = combo['inventario_base'] * SAFETY\n",
    "combo['inventario_ideal'] = np.ceil(combo['inventario_base'] + combo['safety_stock']).astype(int)\n",
    "\n",
    "combo_sorted = combo.sort_values(\n",
    "    ['tienda','score'],\n",
    "    ascending=[True, False]\n",
    ").reset_index(drop=True)\n",
    "\n",
    "combo_topk = combo_sorted.groupby('tienda').head(TOP_K).reset_index(drop=True)\n",
    "\n",
    "try:\n",
    "    worksheet_inventario_tiendas = sheet_parametrizacion.worksheet('inventario_tiendas')\n",
    "    sheet_parametrizacion.del_worksheet(worksheet_inventario_tiendas)\n",
    "except gspread.exceptions.WorksheetNotFound:\n",
    "    print(f\"La hoja 'inventario_tiendas' no existe. Se crear√° una nueva.\")\n",
    "\n",
    "worksheet_inventario_tiendas = sheet_parametrizacion.add_worksheet(title='inventario_tiendas', rows=\"200\", cols=\"20\")\n",
    "\n",
    "combo_topk = combo_topk.fillna(\"\")\n",
    "datos = combo_topk.values.tolist()\n",
    "encabezados = combo_topk.columns.tolist()\n",
    "\n",
    "worksheet_inventario_tiendas.update('A1', [encabezados] + datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb3020d9-89c6-481c-8b30-e76d66c2aca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Encoding aplicado correctamente. Columnas generadas: ['te_raza', 'te_color', 'te_raza_x_tienda', 'te_raza_x_color']\n",
      "Encoders guardados para inferencia: ['raza', 'color', 'raza_x_tienda', 'raza_x_color']\n"
     ]
    }
   ],
   "source": [
    "# Target Encoding (KFold + fallback)\n",
    "\n",
    "df['precio_log'] = np.log1p(df['precio_venta'].fillna(0))\n",
    "\n",
    "categorical_high_card = ['raza', 'color', 'raza_x_tienda', 'raza_x_color']\n",
    "te_cols = [f\"te_{c}\" for c in categorical_high_card]\n",
    "\n",
    "for tc in te_cols:\n",
    "    df[tc] = np.nan\n",
    "\n",
    "SMOOTH_TE = 1.0\n",
    "USE_KFOLD_TE = True\n",
    "N_SPLITS_TE = 5\n",
    "RND = 42\n",
    "\n",
    "TE_ENCODERS = {}\n",
    "\n",
    "# TARGET ENCODING K-FOLD POR COLUMNA\n",
    "if USE_KFOLD_TE:\n",
    "    kf = KFold(n_splits=N_SPLITS_TE, shuffle=True, random_state=RND)\n",
    "\n",
    "    for col in categorical_high_card:\n",
    "        te_name = f\"te_{col}\"\n",
    "        df[te_name] = np.nan\n",
    "\n",
    "        # KFold TE (no hay fuga de informaci√≥n --- leakage-free)\n",
    "        for train_idx, val_idx in kf.split(df):\n",
    "            tr = df.iloc[train_idx]\n",
    "            vl = df.iloc[val_idx]\n",
    "\n",
    "            te = ce.TargetEncoder(cols=[col], smoothing=SMOOTH_TE)\n",
    "            te.fit(tr[[col]], tr['precio_log'])\n",
    "\n",
    "            encoded_vals = te.transform(vl[[col]]).values.flatten()\n",
    "            df.loc[df.index[val_idx], te_name] = encoded_vals\n",
    "\n",
    "        # Fallback global (garantiza que no queden NaN)\n",
    "        te_full_col = ce.TargetEncoder(cols=[col], smoothing=SMOOTH_TE)\n",
    "        te_full_col.fit(df[[col]], df['precio_log'])\n",
    "\n",
    "        fallback_vals = te_full_col.transform(df[[col]]).values.flatten()\n",
    "        mask = df[te_name].isna()\n",
    "        df.loc[mask, te_name] = fallback_vals[mask]\n",
    "\n",
    "        # Guardar encoder final para inferencia\n",
    "        TE_ENCODERS[col] = te_full_col\n",
    "\n",
    "else:\n",
    "    # Sin KFold ‚Üí TE normal\n",
    "    for col in categorical_high_card:\n",
    "        te_full_col = ce.TargetEncoder(cols=[col], smoothing=SMOOTH_TE)\n",
    "        df[f\"te_{col}\"] = te_full_col.fit_transform(df[[col]], df['precio_log']).values.flatten()\n",
    "        TE_ENCODERS[col] = te_full_col\n",
    "\n",
    "print(f\"Target Encoding aplicado correctamente. Columnas generadas: {te_cols}\")\n",
    "print(\"Encoders guardados para inferencia:\", list(TE_ENCODERS.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97e10f1b-604e-4dc6-82c3-a8d3d2f1c3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final feature names (ejemplo): ['precio_mean_tienda', 'prob_vender_tienda', 'tienda_arlington', 'tienda_bedford', 'tienda_desconocido', 'tienda_moore', 'tienda_oklahoma city', 'tienda_plano', 'tienda_rowlett', 'tienda_shenandoah', 'tienda_tulsa', 'genero_Female', 'genero_Female ', 'genero_Male', 'genero_Male ', 'raza_categoria_A', 'raza_categoria_A+', 'raza_categoria_B', 'raza_categoria_C', 'raza_categoria_D', 'raza_categoria_E', 'raza_categoria_F', 'tamano_7-9 lbs', 'tamano_desconocido', 'tamano_medium', 'tamano_micro mini', 'tamano_mini', 'tamano_miniature', 'tamano_standard', 'tamano_toy', 'tamano_ultra mini', 'tamano_xl', 'caracteristica_desconocido', 'caracteristica_f1', 'caracteristica_f1b', 'caracteristica_long hair', 'caracteristica_moyan', 'caracteristica_neutered', 'caracteristica_polydactyl', 'caracteristica_short hair', 'registro_desconocido', 'registro_no', 'registro_si', 'te_raza', 'te_color', 'te_raza_x_tienda', 'te_raza_x_color']\n"
     ]
    }
   ],
   "source": [
    "# ColumnTransformer final y features\n",
    "\n",
    "numeric_features = ['precio_mean_tienda', 'prob_vender_tienda']\n",
    "categorical_low_card = ['tienda', 'genero', 'raza_categoria', 'tamano', 'caracteristica', 'registro']\n",
    "\n",
    "num_transformer = Pipeline([('imputer', SimpleImputer(strategy='median')), ('scaler', StandardScaler())])\n",
    "cat_low_transformer = Pipeline([('imputer', SimpleImputer(strategy='constant', fill_value='MISSING')),\n",
    "                               ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))])\n",
    "\n",
    "# ColumnTransformer: deja pasar las columnas te_... (remainder='passthrough')\n",
    "preproc = ColumnTransformer(transformers=[\n",
    "    ('num', num_transformer, numeric_features),\n",
    "    ('catlow', cat_low_transformer, categorical_low_card)\n",
    "], remainder='passthrough', sparse_threshold=0)\n",
    "\n",
    "# Lista final de features tal como la ver√° el modelo (num + ohe_low + te_cols)\n",
    "# Para obtener nombres reales despu√©s de ajustar preproc, hacemos fit_simple:\n",
    "_sample = df.copy()\n",
    "_prepped = preproc.fit_transform(_sample)\n",
    "# reconstruir los nombres de columnas resultantes\n",
    "ohe_cols = list(preproc.named_transformers_['catlow'].named_steps['ohe'].get_feature_names_out(categorical_low_card))\n",
    "final_feature_names = numeric_features + ohe_cols + te_cols\n",
    "\n",
    "print(\"Final feature names (ejemplo):\", final_feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "448d8668-c47c-4af6-9c7b-db628204a1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Artefactos guardados en: C:\\Users\\alejo\\OneDrive\\Escritorio\\Modelo mascotas\\output\\artefactos_data_engineering.pkl\n"
     ]
    }
   ],
   "source": [
    "# Guardar artefactos data engineering\n",
    "\n",
    "joblib.dump({\n",
    "    'preproc': preproc,\n",
    "    'sc': sc,\n",
    "    'te_encoders': globals().get('TE_ENCODERS', {}),\n",
    "    'final_feature_names': final_feature_names\n",
    "}, ARTIFACTS_DATA_ENG_PATH)\n",
    "print(\"Artefactos guardados en:\", ARTIFACTS_DATA_ENG_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d4199e1-8262-4c64-9942-b54161fed627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (3757, 12) Test: (940, 12)\n"
     ]
    }
   ],
   "source": [
    "# Variables de entrenamiento\n",
    "\n",
    "TARGET = 'precio_log'\n",
    "FEATURES = numeric_features + categorical_low_card + te_cols\n",
    "\n",
    "train_df = df[df['vendido_flag']==1].copy()\n",
    "X = train_df[FEATURES].copy()\n",
    "y = train_df[TARGET].copy()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RND)\n",
    "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "026dbec2-7a07-4420-a812-82d6bb7872da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline + GridSearch\n",
    "\n",
    "if TRAIN:\n",
    "    lgb_model = lgb.LGBMRegressor(\n",
    "        objective='regression',\n",
    "        boosting_type='gbdt',\n",
    "        n_estimators=800,\n",
    "        random_state=RND,\n",
    "        n_jobs=-1,\n",
    "        learning_rate=0.04,\n",
    "        max_depth=8,\n",
    "        num_leaves=60,\n",
    "        min_child_samples=20,\n",
    "        subsample=0.9,\n",
    "        colsample_bytree=0.8,\n",
    "        reg_alpha=0.5,\n",
    "        reg_lambda=1.0\n",
    "    )\n",
    "    \n",
    "    pipe = Pipeline([\n",
    "        ('preproc', preproc),\n",
    "        ('model', lgb_model)\n",
    "    ])\n",
    "    \n",
    "    param_grid = {\n",
    "        'model__num_leaves': [40, 60],\n",
    "        'model__learning_rate': [0.02, 0.04],\n",
    "        'model__min_child_samples': [10, 20],\n",
    "        'model__reg_alpha': [0.3, 0.6],\n",
    "        'model__reg_lambda': [0.5, 1.0]\n",
    "    }\n",
    "    \n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=RND)\n",
    "    \n",
    "    gs = GridSearchCV(pipe, param_grid=param_grid, scoring='neg_mean_absolute_error', cv=cv, verbose=2, n_jobs=-1)\n",
    "    gs.fit(X_train, y_train)\n",
    "    best_model = gs.best_estimator_\n",
    "    print(\"Best params:\", gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46913fe8-9e34-4f42-8258-c427f445500a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    # Determinar el mejor modelo del GridSearch o RandomizedSearch\n",
    "    search_obj = gs if 'gs' in locals() else rs\n",
    "    best_model = search_obj.best_estimator_\n",
    "    \n",
    "    # Asegurar que X_test tiene todas las columnas target-encoded\n",
    "    X_test = train_df.loc[X_test.index, numeric_features + categorical_low_card + te_cols]\n",
    "    \n",
    "    # Predicciones en test\n",
    "    y_pred_log = best_model.predict(X_test)\n",
    "    y_pred = np.expm1(y_pred_log) if np.any(y_test > 0) else y_pred_log\n",
    "    y_true = np.expm1(y_test) if np.any(y_test > 0) else y_test\n",
    "    \n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    print(f\"\\nResultados finales del modelo optimizado:\")\n",
    "    print(f\"MAE  = {mae:.3f}\")\n",
    "    print(f\"RMSE = {rmse:.3f}\")\n",
    "    print(f\"R¬≤   = {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c6c69d04-2d88-42d9-9b4b-7c7748edd1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correcci√≥n de sesgo robusta (usando residuales en train con cross_val_predict)\n",
    "\n",
    "if TRAIN:   \n",
    "    # cross-validated predictions on train to model residuals\n",
    "    cv_preds_log = cross_val_predict(best_model, X_train, y_train, cv=cv, method='predict', n_jobs=-1)\n",
    "    residuals_train = y_train.values - cv_preds_log  # residuals in log space\n",
    "    \n",
    "    # Fit robust regressor (Huber) to predict residual as function of predicted log\n",
    "    corr_model = HuberRegressor().fit(cv_preds_log.reshape(-1,1), residuals_train)\n",
    "    \n",
    "    # Apply correction on test predictions\n",
    "    y_pred_log_corr = y_pred_log + corr_model.predict(y_pred_log.reshape(-1,1))\n",
    "    y_pred_corr_orig = np.expm1(y_pred_log_corr)\n",
    "    \n",
    "    print(\"MAE antes correcci√≥n:\", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"MAE despu√©s correcci√≥n:\", mean_absolute_error(y_true, y_pred_corr_orig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc29e2e6-1f49-4066-8e56-38a4af9b0e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar artefactos\n",
    "\n",
    "if not TRAIN:\n",
    "    artifacts = joblib.load(ARTIFACTS_MODEL_PATH)\n",
    "    best_model = artifacts['best_model']\n",
    "    corr_model = artifacts['corr_model']\n",
    "    metrics = artifacts['metrics']\n",
    "    final_feature_names = artifacts['final_feature_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10c874c7-f308-4247-999d-898b9180d412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones de negocio\n",
    "\n",
    "discount_rules = {}\n",
    "\n",
    "df_descuento_idle_days.columns = df_descuento_idle_days.columns.map(\n",
    "    lambda c: \"\" if pd.isna(c) else str(int(c)) if isinstance(c, float) and c.is_integer() else str(c)\n",
    ")\n",
    "\n",
    "for index, row in df_descuento_idle_days.iterrows():\n",
    "    key = row['Unnamed: 0']\n",
    "    values = [(int(col) if col.isdigit() else col, float(row[col].replace(',', '.'))) for col in df_descuento_idle_days.columns[1:]]\n",
    "    discount_rules[key] = values\n",
    "\n",
    "DEFAULT_FLOOR_PCT = 0.00\n",
    "DEFAULT_MARGIN = 0.00\n",
    "\n",
    "def preparar_fila_inferencia(perro_row, te_encoders,\n",
    "                             numeric_features, categorical_low_card,\n",
    "                             categorical_high_card, te_cols):\n",
    "\n",
    "    if isinstance(perro_row, dict):\n",
    "        row_df = pd.DataFrame([perro_row])\n",
    "    else:\n",
    "        row_df = pd.DataFrame([perro_row.to_dict()])\n",
    "\n",
    "    # Asegurar todas las columnas necesarias\n",
    "    for col in numeric_features + categorical_low_card + categorical_high_card:\n",
    "        if col not in row_df.columns:\n",
    "            row_df[col] = np.nan\n",
    "\n",
    "    # TARGET ENCODING COLUMNA x COLUMNA\n",
    "    for col in categorical_high_card:\n",
    "        enc = te_encoders[col]                # encoder correcto\n",
    "        te_val = enc.transform(row_df[[col]]).values.flatten()\n",
    "        row_df[f\"te_{col}\"] = te_val\n",
    "\n",
    "    # Reordenar columnas seg√∫n pipeline\n",
    "    expected = numeric_features + categorical_low_card + te_cols\n",
    "\n",
    "    return row_df.reindex(columns=expected, fill_value=np.nan)\n",
    "\n",
    "def predict_price_log(single_row_df, model_pipeline, corr_model=None):\n",
    "\n",
    "    if isinstance(single_row_df, dict):\n",
    "        single_row_df = pd.DataFrame([single_row_df])\n",
    "    if isinstance(single_row_df, pd.Series):\n",
    "        single_row_df = single_row_df.to_frame().T\n",
    "\n",
    "    # Eliminar duplicados\n",
    "    single_row_df = single_row_df.loc[:, ~single_row_df.columns.duplicated()]\n",
    "\n",
    "    # Reordenar columnas seg√∫n las del pipeline\n",
    "    if hasattr(model_pipeline, \"feature_names_in_\"):\n",
    "        expected_cols = list(model_pipeline.feature_names_in_)\n",
    "    else:\n",
    "        expected_cols = numeric_features + categorical_low_card + te_cols\n",
    "\n",
    "    single_row_df = single_row_df.reindex(columns=expected_cols, fill_value=np.nan)\n",
    "\n",
    "    pred_log = float(model_pipeline.predict(single_row_df)[0])\n",
    "\n",
    "    if corr_model is not None:\n",
    "        pred_log += float(corr_model.predict(np.array(pred_log).reshape(-1, 1))[0])\n",
    "\n",
    "    return pred_log\n",
    "\n",
    "def calcular_precio_base_contextual(row, df_base):\n",
    "    \n",
    "    raza = row.get('raza')\n",
    "    color = row.get('color')\n",
    "    cat = row.get('raza_categoria')\n",
    "    tienda = row.get('tienda')\n",
    "    genero = row.get('genero')\n",
    "\n",
    "    # raza + color + genero\n",
    "    if raza and color and genero:\n",
    "        sub = df_base[(df_base['raza'] == raza) & (df_base['color'] == color) & (df_base['genero'] == genero) & df_base['precio_venta'].notna()]\n",
    "        if len(sub):\n",
    "            return float(sub['precio_venta'].median())\n",
    "\n",
    "    # raza sola\n",
    "    sub = df_base[(df_base['raza'] == raza) & df_base['precio_venta'].notna()]\n",
    "    if len(sub):\n",
    "        return float(sub['precio_venta'].median())\n",
    "\n",
    "    # categor√≠a de raza\n",
    "    if cat:\n",
    "        sub = df_base[(df_base['raza_categoria'] == cat) & df_base['precio_venta'].notna()]\n",
    "        if len(sub):\n",
    "            return float(sub['precio_venta'].median())\n",
    "\n",
    "    # tienda\n",
    "    if tienda:\n",
    "        sub = df_base[(df_base['tienda'] == tienda) & df_base['precio_venta'].notna()]\n",
    "        if len(sub):\n",
    "            return float(sub['precio_venta'].median())\n",
    "\n",
    "    # global\n",
    "    global_mean = df_base['precio_venta'].median()\n",
    "    return float(global_mean if pd.notna(global_mean) else 0.0)\n",
    "\n",
    "\n",
    "def sugerir_precio_dinamico(perro_row, model_pipeline, te_full, clf_pipeline=None, corr_model=None, default_margin=DEFAULT_MARGIN):\n",
    "    \n",
    "    r = perro_row.to_dict() if isinstance(perro_row, pd.Series) else dict(perro_row)\n",
    "\n",
    "    X_row = preparar_fila_inferencia(r, te_full, numeric_features, categorical_low_card, categorical_high_card, te_cols)\n",
    "    \n",
    "    pred_log = predict_price_log(X_row, model_pipeline, corr_model=corr_model)\n",
    "    pred_price = float(np.expm1(pred_log))\n",
    "\n",
    "    if pd.notna(r.get('base')):\n",
    "        precio_hist = float(r['base'])\n",
    "    else:\n",
    "        precio_hist = calcular_precio_base_contextual(r, train_df)\n",
    "\n",
    "    dias_val = r.get('dias_en_tienda', 0)\n",
    "    dias = int(0 if pd.isna(dias_val) else dias_val)\n",
    "    vistas_val = r.get('vistas_7d', 0)\n",
    "    vistas = int(0 if pd.isna(vistas_val) else vistas_val)\n",
    "\n",
    "    categoria = r.get('raza_categoria', None)\n",
    "    raza = r.get('raza', None)\n",
    "\n",
    "    descuento = 0.0\n",
    "    if raza == 'dachshund' and dias < 28:\n",
    "        descuento = descuento\n",
    "    elif categoria in discount_rules:\n",
    "        for d, desc in sorted(discount_rules[categoria]):\n",
    "            if dias >= d:\n",
    "                descuento += desc\n",
    "    else:\n",
    "        descuento = descuento\n",
    "\n",
    "    demanda_adj = 0#-0.05 if vistas < 10 else (0.10 if vistas >= 100 else (0.05 if vistas >= 50 else 0.0))\n",
    "\n",
    "    precio_desc = pred_price * (1 - descuento)\n",
    "    precio_sugerido = precio_desc * (1 + demanda_adj + default_margin)\n",
    "\n",
    "    min_val = pd.to_numeric(r.get('min'), errors='coerce')\n",
    "    if pd.notna(min_val):\n",
    "        floor_price = min_val\n",
    "    else:\n",
    "        floor_price = precio_hist * DEFAULT_FLOOR_PCT\n",
    "\n",
    "    ajuste_tipo = None\n",
    "    if precio_sugerido < floor_price:\n",
    "        precio_sugerido = floor_price\n",
    "        ajuste_tipo = 'floor_aplicado'\n",
    "\n",
    "    return {\n",
    "        'precio_modelo': round(pred_price, 2),\n",
    "        'precio_sugerido': round(precio_sugerido, 2),\n",
    "        'applied_discount_pct': round(descuento, 4),\n",
    "        'demand_adj_pct': round(demanda_adj, 4),\n",
    "        'dias_en_tienda': dias,\n",
    "        'categoria': categoria,\n",
    "        'vistas_7d': vistas,\n",
    "        'ajuste_aplicado': bool(ajuste_tipo),\n",
    "        'tipo_ajuste': ajuste_tipo,\n",
    "        'precio_base': precio_hist\n",
    "    }\n",
    "\n",
    "def recomendar_reubicacion(perro_row, tiendas_list, model_pipeline, te_full, clf_pipeline=None, corr_model=None, top_k=3):\n",
    "\n",
    "    resultados = []\n",
    "    for t in tiendas_list:\n",
    "        \n",
    "        perro_row['tienda'] = t\n",
    "        perro_row_df = perro_row.to_frame().T\n",
    "        perro_row_df = perro_row_df.drop(columns=['precio_mean_tienda', 'prob_vender_tienda', 'raza_x_tienda'])\n",
    "        perro_row_df = perro_row_df.merge(agg_t[['tienda', 'precio_mean_tienda', 'prob_vender_tienda']], on='tienda', how='left')\n",
    "        perro_row_df['raza_x_tienda'] = perro_row_df['raza'].astype(str) + '___' + perro_row_df['tienda'].astype(str)\n",
    "        perro_row = perro_row_df.iloc[0]\n",
    "    \n",
    "        X_row = preparar_fila_inferencia(perro_row, te_full, numeric_features, categorical_low_card, categorical_high_card, te_cols)\n",
    "   \n",
    "        try:\n",
    "            p_log = predict_price_log(X_row, model_pipeline, corr_model=corr_model)\n",
    "            p = float(np.expm1(p_log))\n",
    "        except Exception:\n",
    "            p = np.nan\n",
    "        X_row['dias_en_tienda'] = perro_row.get('dias_en_tienda')\n",
    "        \n",
    "        prob = 1\n",
    "        score = p * prob if (not pd.isna(p) and not pd.isna(prob)) else np.nan\n",
    "        resultados.append({'tienda': t, 'precio_esperado': p, 'prob_venta': prob, 'score': score})\n",
    "    return pd.DataFrame(resultados).sort_values('score', ascending=False).head(top_k)\n",
    "\n",
    "def aproximar_precio(precio_venta, precios_disponibles, precio_base):\n",
    "\n",
    "    precios_disponibles = sorted(precios_disponibles)\n",
    "    \n",
    "    if precio_venta <= precios_disponibles[0]:\n",
    "        return precios_disponibles[0]\n",
    "    \n",
    "    if precio_venta >= precios_disponibles[-1]:\n",
    "        return precios_disponibles[-1]\n",
    "\n",
    "    for i in range(len(precios_disponibles)-1):\n",
    "        p_inf = precios_disponibles[i]\n",
    "        p_sup = precios_disponibles[i+1]\n",
    "        \n",
    "        # Validamos si el precio_venta cae en el intervalo actual\n",
    "        if p_inf <= precio_venta <= p_sup:\n",
    "            # Calcular el descuento relativo al precio_base para el precio inferior\n",
    "            descuento_inf = (precio_base - p_inf) / precio_base\n",
    "\n",
    "            # Definir el umbral_relativo seg√∫n el descuento\n",
    "            if descuento_inf <= 0.10:\n",
    "                umbral_relativo = 0.05  # 5% de umbral si el descuento es 10% o menor\n",
    "            elif descuento_inf <= 0.20:\n",
    "                umbral_relativo = 0.06  # 6% de umbral si el descuento es 20% o menor\n",
    "            elif descuento_inf <= 0.25:\n",
    "                umbral_relativo = 0.08  # 8% de umbral si el descuento es 25% o menor\n",
    "            elif descuento_inf <= 0.35:\n",
    "                umbral_relativo = 0.10  # 10% de umbral si el descuento es 35% o menor\n",
    "            else:\n",
    "                umbral_relativo = 0.12  # 12% de umbral si el descuento es mayor a 35%\n",
    "\n",
    "            # Condici√≥n para verificar si el precio_venta est√° cerca del precio inferior\n",
    "            if (precio_venta - p_inf) / p_inf <= umbral_relativo:\n",
    "                return p_inf\n",
    "            # Si el precio_venta no est√° cerca del precio inferior, usamos el precio superior\n",
    "            else:\n",
    "                return p_sup\n",
    "    \n",
    "    # Si no se encuentra en ning√∫n intervalo, se usa el precio m√°s cercano\n",
    "    return precios_disponibles[-1]  # O el precio m√°s cercano seg√∫n el caso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "018624a2-fe78-4098-9677-1ad2c7b95be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Exportado correctamente: C:\\Users\\alejo\\OneDrive\\Escritorio\\Modelo mascotas\\output\\recomendaciones_precios_20260107_074317.csv\n",
      "Filas generadas: 355\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>raza</th>\n",
       "      <th>color</th>\n",
       "      <th>genero</th>\n",
       "      <th>tamano</th>\n",
       "      <th>caracteristica</th>\n",
       "      <th>registro</th>\n",
       "      <th>precio_base</th>\n",
       "      <th>precio_venta_hist</th>\n",
       "      <th>precio_modelo</th>\n",
       "      <th>...</th>\n",
       "      <th>demand_adj_pct</th>\n",
       "      <th>tienda_actual</th>\n",
       "      <th>tienda_recomendada</th>\n",
       "      <th>raza_categoria</th>\n",
       "      <th>dias_en_tienda</th>\n",
       "      <th>vistas_7d</th>\n",
       "      <th>fecha_generacion</th>\n",
       "      <th>ajuste_aplicado</th>\n",
       "      <th>tipo_ajuste</th>\n",
       "      <th>comentario</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>932002000719574</td>\n",
       "      <td>dachshund</td>\n",
       "      <td>chocolate &amp; tan</td>\n",
       "      <td>Female</td>\n",
       "      <td>desconocido</td>\n",
       "      <td>short hair</td>\n",
       "      <td>si</td>\n",
       "      <td>3475</td>\n",
       "      <td>1625</td>\n",
       "      <td>3176</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>arlington</td>\n",
       "      <td>moore</td>\n",
       "      <td>A+</td>\n",
       "      <td>168</td>\n",
       "      <td>None</td>\n",
       "      <td>2026-01-07 07:44:06</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>900263002263978</td>\n",
       "      <td>shih tzu</td>\n",
       "      <td>red &amp; white</td>\n",
       "      <td>Female</td>\n",
       "      <td>desconocido</td>\n",
       "      <td>desconocido</td>\n",
       "      <td>si</td>\n",
       "      <td>3200</td>\n",
       "      <td>2400</td>\n",
       "      <td>2937</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>oklahoma city</td>\n",
       "      <td>rowlett</td>\n",
       "      <td>B</td>\n",
       "      <td>161</td>\n",
       "      <td>None</td>\n",
       "      <td>2026-01-07 07:44:07</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>932002000723069</td>\n",
       "      <td>dachshund</td>\n",
       "      <td>red &amp; sable</td>\n",
       "      <td>Male</td>\n",
       "      <td>desconocido</td>\n",
       "      <td>long hair</td>\n",
       "      <td>si</td>\n",
       "      <td>3475</td>\n",
       "      <td>2200</td>\n",
       "      <td>3324</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>plano</td>\n",
       "      <td>moore</td>\n",
       "      <td>A+</td>\n",
       "      <td>161</td>\n",
       "      <td>None</td>\n",
       "      <td>2026-01-07 07:44:07</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>932002000736271</td>\n",
       "      <td>aussiedoodle</td>\n",
       "      <td>brown &amp; white</td>\n",
       "      <td>Female</td>\n",
       "      <td>toy</td>\n",
       "      <td>desconocido</td>\n",
       "      <td>no</td>\n",
       "      <td>2825</td>\n",
       "      <td>2475</td>\n",
       "      <td>2866</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>bedford</td>\n",
       "      <td>moore</td>\n",
       "      <td>C</td>\n",
       "      <td>135</td>\n",
       "      <td>None</td>\n",
       "      <td>2026-01-07 07:44:07</td>\n",
       "      <td>True</td>\n",
       "      <td>floor_aplicado</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>932002000721642</td>\n",
       "      <td>dachshund</td>\n",
       "      <td>otros</td>\n",
       "      <td>Male</td>\n",
       "      <td>desconocido</td>\n",
       "      <td>desconocido</td>\n",
       "      <td>si</td>\n",
       "      <td>3475</td>\n",
       "      <td>1800</td>\n",
       "      <td>3437</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>oklahoma city</td>\n",
       "      <td>moore</td>\n",
       "      <td>A+</td>\n",
       "      <td>125</td>\n",
       "      <td>None</td>\n",
       "      <td>2026-01-07 07:44:08</td>\n",
       "      <td>False</td>\n",
       "      <td>None</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                id          raza            color  genero       tamano  \\\n",
       "0  932002000719574     dachshund  chocolate & tan  Female  desconocido   \n",
       "1  900263002263978      shih tzu      red & white  Female  desconocido   \n",
       "2  932002000723069     dachshund      red & sable    Male  desconocido   \n",
       "3  932002000736271  aussiedoodle    brown & white  Female          toy   \n",
       "4  932002000721642     dachshund            otros    Male  desconocido   \n",
       "\n",
       "  caracteristica registro  precio_base  precio_venta_hist  precio_modelo  ...  \\\n",
       "0     short hair       si         3475               1625           3176  ...   \n",
       "1    desconocido       si         3200               2400           2937  ...   \n",
       "2      long hair       si         3475               2200           3324  ...   \n",
       "3    desconocido       no         2825               2475           2866  ...   \n",
       "4    desconocido       si         3475               1800           3437  ...   \n",
       "\n",
       "   demand_adj_pct  tienda_actual  tienda_recomendada  raza_categoria  \\\n",
       "0               0      arlington               moore              A+   \n",
       "1               0  oklahoma city             rowlett               B   \n",
       "2               0          plano               moore              A+   \n",
       "3               0        bedford               moore               C   \n",
       "4               0  oklahoma city               moore              A+   \n",
       "\n",
       "  dias_en_tienda vistas_7d     fecha_generacion  ajuste_aplicado  \\\n",
       "0            168      None  2026-01-07 07:44:06            False   \n",
       "1            161      None  2026-01-07 07:44:07            False   \n",
       "2            161      None  2026-01-07 07:44:07            False   \n",
       "3            135      None  2026-01-07 07:44:07             True   \n",
       "4            125      None  2026-01-07 07:44:08            False   \n",
       "\n",
       "      tipo_ajuste comentario  \n",
       "0            None             \n",
       "1            None             \n",
       "2            None             \n",
       "3  floor_aplicado             \n",
       "4            None             \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Exportar recomendaciones (no vendidos)\n",
    "\n",
    "#unv = predict_df[predict_df['last']==1].copy().reset_index(drop=True)\n",
    "#.loc[predict_df[\"id\"].isin([\"932002000719523\",\"900255002110908\"])].head(2)\n",
    "unv = predict_df.copy().reset_index(drop=True)\n",
    "unv = unv.loc[:, ~unv.columns.duplicated()].copy()\n",
    "\n",
    "tiendas_list = df['tienda'].unique().tolist()\n",
    "rows = []\n",
    "# Limpieza preventiva: corregir categor√≠as duplicadas en OHE\n",
    "ohe = best_model.named_steps['preproc'].named_transformers_['catlow'].named_steps['ohe']\n",
    "if hasattr(ohe, 'categories_'):\n",
    "    for i, cats in enumerate(ohe.categories_):\n",
    "        ohe.categories_[i] = np.unique(cats)\n",
    "\n",
    "def safe_int_convert(value):\n",
    "    if pd.notna(value):\n",
    "        return int(float(value))\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "for _, perro in unv.iterrows():\n",
    "    tienda_orig = ''\n",
    "    \n",
    "    tienda_orig = perro['tienda']\n",
    "    reloc = recomendar_reubicacion(perro, tiendas_list, best_model, TE_ENCODERS, corr_model=corr_model, top_k=1)\n",
    " \n",
    "    if not reloc.empty:\n",
    "        tienda_recom = reloc.iloc[0]['tienda']\n",
    "        prob_recom = float(reloc.iloc[0]['prob_venta']) if not pd.isna(reloc.iloc[0]['prob_venta']) else np.nan\n",
    "    else:\n",
    "        tienda_recom = None\n",
    "        prob_recom = np.nan\n",
    "   \n",
    "    perro['tienda'] = tienda_recom\n",
    "    \n",
    "    if isinstance(perro, pd.Series):\n",
    "        perro = perro.to_frame().T\n",
    "    \n",
    "    perro = perro.drop(['precio_mean_tienda','prob_vender_tienda','raza_x_tienda'], axis=1)\n",
    "\n",
    "    perro = perro.merge(\n",
    "        agg_t[['tienda','precio_mean_tienda','prob_vender_tienda']],\n",
    "        on='tienda',\n",
    "        how='left'\n",
    "    )\n",
    "    \n",
    "    perro['raza_x_tienda'] = perro['raza'].astype(str) + '___' + perro['tienda'].astype(str)\n",
    "    perro = perro.iloc[0]\n",
    "  \n",
    "    sug = sugerir_precio_dinamico(perro, best_model, TE_ENCODERS, corr_model=corr_model)\n",
    "\n",
    "    valores = [desc for (_, desc) in discount_rules[perro.get('raza_categoria')]]\n",
    "    base = float(perro.get('base'))\n",
    "    acumulado = 0\n",
    "    precios_disponibles = []\n",
    "    for v in valores:\n",
    "        acumulado += v\n",
    "        precios_disponibles.append(base * (1 - acumulado))\n",
    "    precio_aprox = aproximar_precio(sug['precio_sugerido'], precios_disponibles, base)\n",
    "\n",
    "    rows.append({\n",
    "        'id': perro.get('id'),\n",
    "        'raza': perro.get('raza'),\n",
    "        'color': perro.get('color'),\n",
    "        'genero': perro.get('genero'),\n",
    "        'tamano': perro.get('tamano'),\n",
    "        'caracteristica': perro.get('caracteristica'),\n",
    "        'registro': perro.get('registro'),\n",
    "        'precio_base': safe_int_convert(sug['precio_base']) if pd.notna(sug['precio_base']) else np.nan,\n",
    "        'precio_venta_hist': safe_int_convert(perro.get('precio_venta', np.nan)) if pd.notna(perro.get('precio_venta')) else np.nan,\n",
    "        'precio_modelo': safe_int_convert(sug['precio_modelo']) if pd.notna(sug['precio_modelo']) else np.nan,\n",
    "        'precio_sugerido': safe_int_convert(sug['precio_sugerido']) if pd.notna(sug['precio_sugerido']) else np.nan,\n",
    "        'precio_aproximado': safe_int_convert(precio_aprox) if pd.notna(precio_aprox) else np.nan,\n",
    "        'applied_discount_pct': sug['applied_discount_pct'] * -1,\n",
    "        'demand_adj_pct': sug['demand_adj_pct'],\n",
    "        'tienda_actual': tienda_orig,\n",
    "        'tienda_recomendada': tienda_recom,\n",
    "        'raza_categoria': perro.get('raza_categoria'),\n",
    "        'dias_en_tienda': perro.get('dias_en_tienda'),\n",
    "        'vistas_7d': perro.get('vistas_7d'),\n",
    "        'fecha_generacion': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'ajuste_aplicado': sug['ajuste_aplicado'],\n",
    "        'tipo_ajuste': sug['tipo_ajuste'],\n",
    "        'comentario': perro.get('comentario')\n",
    "    })\n",
    "\n",
    "recs_df = pd.DataFrame(rows)\n",
    "numeric_cols = ['precio_modelo', 'precio_sugerido', 'applied_discount_pct', 'demand_adj_pct', 'precio_venta_hist']\n",
    "recs_df[numeric_cols] = recs_df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "worksheet_ejecucion_recomendaciones = sheet_ejecucion.add_worksheet(title=f'recomendaciones_{fecha_str}', rows=\"10000\", cols=\"50\")\n",
    "recs_df_send = recs_df.fillna(\"\").copy()\n",
    "datos = recs_df_send.values.tolist()\n",
    "encabezados = recs_df_send.columns.tolist()\n",
    "worksheet_ejecucion_recomendaciones.update('A1', [encabezados] + datos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b2765683-a731-4c5f-b95c-4f8bb2ced6ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úî Actualizaci√≥n completada (1026 celdas)\n"
     ]
    }
   ],
   "source": [
    "# Actualizar Google Sheet\n",
    "\n",
    "def safe_float(x):\n",
    "    try:\n",
    "        x_str = str(x).strip()\n",
    "        if x_str == \"\":\n",
    "            return np.nan\n",
    "        return float(x_str)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def safe_int_convert(x):\n",
    "    try:\n",
    "        if pd.isna(x):\n",
    "            return \"\"\n",
    "        return int(float(x))\n",
    "    except:\n",
    "        return \"\"\n",
    "        \n",
    "def actualizar_sheet_incremental_batch(worksheet, gs_df, recs_df):\n",
    "\n",
    "    header = worksheet.row_values(1)\n",
    "    header_map = {c: i + 1 for i, c in enumerate(header)}\n",
    "\n",
    "    if \"Price\" not in header:\n",
    "        raise ValueError(\"‚ùå La hoja no tiene columna 'Price'\")\n",
    "\n",
    "    price_main_col = header_map[\"Price\"]\n",
    "\n",
    "    updates = []\n",
    "    new_columns = []\n",
    "\n",
    "    for _, row in recs_df.iterrows():\n",
    "        perro_id = str(row[\"id\"]).strip().lower()\n",
    "        nuevo_precio = safe_float(row[\"precio_aproximado\"])\n",
    "        fecha_actual = row[\"fecha_generacion\"]\n",
    "\n",
    "        if np.isnan(nuevo_precio):\n",
    "            continue\n",
    "\n",
    "        match = gs_df[\n",
    "            gs_df[\"Microchip #\"]\n",
    "            .astype(str)\n",
    "            .str.lower()\n",
    "            .str.strip()\n",
    "            == perro_id\n",
    "        ]\n",
    "\n",
    "        if match.empty:\n",
    "            continue\n",
    "\n",
    "        idx = match.index[0]\n",
    "        sheet_row = idx + 2  # header ocupa fila 1\n",
    "\n",
    "        # SIEMPRE actualizar Price (precio vigente)\n",
    "        updates.append(\n",
    "            gspread.Cell(\n",
    "                row=sheet_row,\n",
    "                col=price_main_col,\n",
    "                value=safe_int_convert(nuevo_precio)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # HIST√ìRICO SOLO SI CAMBIA\n",
    "        price_cols = sorted(\n",
    "            [c for c in gs_df.columns if c.startswith(\"Price_\")],\n",
    "            key=lambda x: int(x.split(\"_\")[1])\n",
    "        )\n",
    "\n",
    "        last_price_val = np.nan\n",
    "        for col in reversed(price_cols):\n",
    "            val = safe_float(gs_df.loc[idx, col])\n",
    "            if not np.isnan(val):\n",
    "                last_price_val = val\n",
    "                break\n",
    "\n",
    "        # Si es el mismo precio ‚Üí NO crear hist√≥rico\n",
    "        if not np.isnan(last_price_val) and last_price_val == nuevo_precio:\n",
    "            continue\n",
    "\n",
    "        # Buscar slot libre o crear siguiente\n",
    "        next_n = 1\n",
    "        for col in price_cols:\n",
    "            if pd.isna(gs_df.loc[idx, col]) or str(gs_df.loc[idx, col]).strip() == \"\":\n",
    "                next_n = int(col.split(\"_\")[1])\n",
    "                break\n",
    "        else:\n",
    "            if price_cols:\n",
    "                next_n = int(price_cols[-1].split(\"_\")[1]) + 1\n",
    "\n",
    "        price_col = f\"Price_{next_n:02d}\"\n",
    "        date_col  = f\"Date_{next_n:02d}\"\n",
    "\n",
    "        # Crear columnas si no existen\n",
    "        for col in (price_col, date_col):\n",
    "            if col not in header:\n",
    "                new_columns.append(col)\n",
    "                header.append(col)\n",
    "                header_map[col] = len(header)\n",
    "\n",
    "        # Registrar hist√≥rico\n",
    "        updates.append(\n",
    "            gspread.Cell(\n",
    "                row=sheet_row,\n",
    "                col=header_map[price_col],\n",
    "                value=safe_int_convert(nuevo_precio)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        updates.append(\n",
    "            gspread.Cell(\n",
    "                row=sheet_row,\n",
    "                col=header_map[date_col],\n",
    "                value=datetime.strptime(\n",
    "                    fecha_actual, \"%Y-%m-%d %H:%M:%S\"\n",
    "                ).strftime(\"%d-%m-%Y\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "    # Crear columnas nuevas una sola vez\n",
    "    if new_columns:\n",
    "        worksheet.add_cols(len(new_columns))\n",
    "        worksheet.update('1:1', [header])\n",
    "\n",
    "    # Batch update\n",
    "    if updates:\n",
    "        worksheet.update_cells(updates)\n",
    "        print(f\"‚úî Actualizaci√≥n completada ({len(updates)} celdas)\")\n",
    "    else:\n",
    "        print(\"‚Ñπ No hubo actualizaciones\")\n",
    "\n",
    "def actualizar_sheet_price_batch(worksheet, gs_df, recs_df):\n",
    "\n",
    "    header = worksheet.row_values(1)\n",
    "    header_map = {c: i + 1 for i, c in enumerate(header)}\n",
    "\n",
    "    if \"Microchip#\" not in header:\n",
    "        raise ValueError(\"‚ùå La hoja no tiene columna 'Microchip #'\")\n",
    "\n",
    "    if \"Price\" not in header:\n",
    "        raise ValueError(\"‚ùå La hoja no tiene columna 'Price'\")\n",
    "\n",
    "    price_col_idx = header_map[\"Price\"]\n",
    "\n",
    "    date_col_idx = header_map.get(\"Date\", None)\n",
    "\n",
    "    updates = []\n",
    "\n",
    "    for _, row in recs_df.iterrows():\n",
    "        perro_id = str(row[\"id\"]).strip().lower()\n",
    "        nuevo_precio = safe_float(row[\"precio_aproximado\"])\n",
    "\n",
    "        if np.isnan(nuevo_precio):\n",
    "            continue\n",
    "\n",
    "        match = gs_df[\n",
    "            gs_df[\"Microchip#\"]\n",
    "            .astype(str)\n",
    "            .str.strip()\n",
    "            .str.lower()\n",
    "            == perro_id\n",
    "        ]\n",
    "\n",
    "        if match.empty:\n",
    "            continue\n",
    "\n",
    "        idx = match.index[0]\n",
    "        sheet_row = idx + 2\n",
    "\n",
    "        precio_actual = safe_float(gs_df.loc[idx, \"Price\"])\n",
    "\n",
    "        if not np.isnan(precio_actual) and precio_actual == nuevo_precio:\n",
    "            continue\n",
    "\n",
    "        updates.append(\n",
    "            gspread.Cell(\n",
    "                row=sheet_row,\n",
    "                col=price_col_idx,\n",
    "                value=safe_int_convert(nuevo_precio)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if date_col_idx:\n",
    "            updates.append(\n",
    "                gspread.Cell(\n",
    "                    row=sheet_row,\n",
    "                    col=date_col_idx,\n",
    "                    value=datetime.now().strftime(\"%d-%m-%Y\")\n",
    "                )\n",
    "            )\n",
    "\n",
    "    # Ejecutar batch update\n",
    "    if updates:\n",
    "        worksheet.update_cells(updates)\n",
    "        print(f\"‚úî {len(updates)} celdas actualizadas\")\n",
    "    else:\n",
    "        print(\"‚Ñπ No hubo cambios de precio\")\n",
    "\n",
    "if NEW_RECORDS:\n",
    "    actualizar_sheet_price_batch(worksheet_new, df_sheet_new, recs_df)\n",
    "else:\n",
    "    actualizar_sheet_incremental_batch(worksheet, df_sheet, recs_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e4ae3861-b6d2-4f50-9f9d-f6a5199f5f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nenviar_correo(\\n    \"üß™ Test correo autom√°tico\",\\n    \"Si ves esto, el sistema de notificaciones funciona.\"\\n)'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''def pipeline_completo:\n",
    "    ...\n",
    "\n",
    "try:\n",
    "    \n",
    "    print(\"‚úÖ Proceso ejecutado correctamente\")\n",
    "\n",
    "    enviar_correo(\n",
    "        asunto=\"‚úÖ Modelo Mascotas ‚Äî Ejecuci√≥n Exitosa\",\n",
    "        mensaje=f\"\"\"\n",
    "El modelo se ejecut√≥ correctamente.\n",
    "\n",
    "Fecha: {datetime.now().strftime('%d-%m-%Y %H:%M:%S')}\n",
    "Estado: OK\n",
    "        \"\"\"\n",
    "    )\n",
    "\n",
    "except Exception as e:\n",
    "    error_trace = traceback.format_exc()\n",
    "\n",
    "    enviar_correo(\n",
    "        asunto=\"‚ùå Modelo Mascotas ‚Äî ERROR en Ejecuci√≥n\",\n",
    "        mensaje=f\"\"\"\n",
    "El modelo fall√≥ durante la ejecuci√≥n.\n",
    "\n",
    "Fecha: {datetime.now().strftime('%d-%m-%Y %H:%M:%S')}\n",
    "\n",
    "Error:\n",
    "{str(e)}\n",
    "\n",
    "Traceback:\n",
    "{error_trace}\n",
    "        \"\"\"\n",
    "    )\n",
    "    raise  # importante para que Task Scheduler / Airflow marque error'''\n",
    "    \n",
    "'''\n",
    "enviar_correo(\n",
    "    \"üß™ Test correo autom√°tico\",\n",
    "    \"Si ves esto, el sistema de notificaciones funciona.\"\n",
    ")'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac6dd1eb-d89f-4132-a49c-7824af2fa156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hoja1 449624\n",
      "Hoja2 461776\n",
      "Hoja5 26000\n",
      "Copy of Copy of Copy of Hoja2 375624\n",
      "TOTAL: 1313024\n"
     ]
    }
   ],
   "source": [
    "spreadsheet = worksheet.spreadsheet\n",
    "\n",
    "total_cells = 0\n",
    "for ws in spreadsheet.worksheets():\n",
    "    cells = ws.row_count * ws.col_count\n",
    "    print(ws.title, cells)\n",
    "    total_cells += cells\n",
    "\n",
    "print(\"TOTAL:\", total_cells)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "418254c8-7e51-4233-9931-b56463584a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "461776\n"
     ]
    }
   ],
   "source": [
    "rows = worksheet.row_count\n",
    "cols = worksheet.col_count\n",
    "print(rows * cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "596d78ef-34b5-4c4c-b010-df7a8386f57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar artefactos y metricas metricas\n",
    "\n",
    "if TRAIN:\n",
    "    metrics = {\n",
    "        'mae_before_correction': float(mean_absolute_error(y_true, y_pred)),\n",
    "        'mae_after_correction': float(mean_absolute_error(y_true, y_pred_corr_orig)),\n",
    "        'rmse': float(rmse),\n",
    "        'r2': float(r2)\n",
    "    }\n",
    "    \n",
    "    joblib.dump({\n",
    "        'best_model': best_model,\n",
    "        #'clf_pipe': best_clf,\n",
    "        'corr_model': corr_model,\n",
    "        'metrics': metrics,\n",
    "        'final_feature_names': final_feature_names \n",
    "    }, ARTIFACTS_MODEL_PATH)\n",
    "    \n",
    "    print(\"Artifacts guardados en:\", ARTIFACTS_MODEL_PATH)\n",
    "    print(\"Metrics:\", metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54c9c1f4-adbe-40ef-b57a-483655a2846e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluaci√≥n, Importancia de Caracter√≠sticas y Nombres Reales\n",
    "\n",
    "if TRAIN:\n",
    "    # Determinar el mejor modelo del GridSearch o RandomizedSearch\n",
    "    search_obj = gs if 'gs' in locals() else rs\n",
    "    best_model = search_obj.best_estimator_\n",
    "    \n",
    "    # EVALUACI√ìN DEL MODELO\n",
    "    # Asegurarse de que X_test tiene todas las columnas target-encoded\n",
    "    X_test = df.loc[X_test.index, numeric_features + categorical_low_card + te_cols]\n",
    "    \n",
    "    # Predicciones en escala logar√≠tmica (ya que el modelo fue entrenado en log)\n",
    "    y_pred_log = best_model.predict(X_test)\n",
    "    \n",
    "    # M√©tricas en log\n",
    "    rmse_log = np.sqrt(mean_squared_error(y_test, y_pred_log))  # Ra√≠z cuadrada de error cuadr√°tico\n",
    "    mae_log = np.mean(np.abs(y_test - y_pred_log))  # Error absoluto medio\n",
    "    r2_log = r2_score(y_test, y_pred_log)  # R¬≤ en escala logar√≠tmica\n",
    "    \n",
    "    # M√©tricas en escala real (deshacer la transformaci√≥n logar√≠tmica)\n",
    "    y_pred_real = np.expm1(y_pred_log)  # Recuperar los valores en la escala real\n",
    "    y_true_real = np.expm1(y_test)  # Tambi√©n para los valores reales\n",
    "    \n",
    "    rmse_real = np.sqrt(mean_squared_error(y_true_real, y_pred_real))  # RMSE en escala real\n",
    "    mae_real = np.mean(np.abs(y_true_real - y_pred_real))  # MAE en escala real\n",
    "    r2_real = r2_score(y_true_real, y_pred_real)  # R¬≤ en escala real\n",
    "    \n",
    "    # Imprimir resultados\n",
    "    print(f\"Resultados del Modelo Optimizado:\")\n",
    "    print(f\"RMSE (log)  = {rmse_log:.4f}\")\n",
    "    print(f\"MAE (log)   = {mae_log:.4f}\")\n",
    "    print(f\"R¬≤ (log)    = {r2_log:.3f}\")\n",
    "    print(f\"RMSE (real) = {rmse_real:.2f}\")\n",
    "    print(f\"MAE (real)  = {mae_real:.2f}\")\n",
    "    print(f\"R¬≤ (real)   = {r2_real:.3f}\")\n",
    "    \n",
    "    # GRAFICOS DE DIAGN√ìSTICO\n",
    "    # --- 1. Real vs Predicho ---\n",
    "    plt.figure(figsize=(6,6))\n",
    "    sns.scatterplot(x=y_true_real, y=y_pred_real, alpha=0.6)\n",
    "    plt.plot([y_true_real.min(), y_true_real.max()], [y_true_real.min(), y_true_real.max()], 'r--', label='Ideal')\n",
    "    plt.xlabel(\"Valor Real\")\n",
    "    plt.ylabel(\"Predicci√≥n Modelo\")\n",
    "    plt.title(\"Predicci√≥n vs Valor Real\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # --- 2. Distribuci√≥n de errores ---\n",
    "    residuals = y_true_real - y_pred_real\n",
    "    plt.figure(figsize=(8,5))\n",
    "    sns.histplot(residuals, bins=30, kde=True)\n",
    "    plt.title(\"Distribuci√≥n de Errores (Residuos)\")\n",
    "    plt.xlabel(\"Error (Valor Real - Predicho)\")\n",
    "    plt.ylabel(\"Frecuencia\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # IMPORTANCIA DE VARIABLES (LightGBM)\n",
    "    try:\n",
    "        # Acceder al modelo LightGBM dentro del Pipeline\n",
    "        booster = best_model.named_steps['model'].booster_\n",
    "        importance = booster.feature_importance(importance_type='gain')\n",
    "        feature_names = booster.feature_name()\n",
    "    \n",
    "        # Si los nombres de las caracter√≠sticas son gen√©ricos (ej. Column_0, Column_1...), reconstruir los nombres reales\n",
    "        if feature_names == ['Column_' + str(i) for i in range(len(feature_names))]:\n",
    "            # Reconstruir los nombres de las caracter√≠sticas si hay un preprocesador\n",
    "            if 'preproc' in best_model.named_steps:\n",
    "                preprocessor = best_model.named_steps['preproc']\n",
    "                num_names = numeric_features  # Asumiendo que ya tienes la lista de features num√©ricas\n",
    "    \n",
    "                # Obtener los nombres de las columnas codificadas\n",
    "                ohe_transformer = preprocessor.named_transformers_['catlow']['ohe']\n",
    "                ohe_cols = list(ohe_transformer.get_feature_names_out(categorical_low_card))\n",
    "                high_card_names = categorical_high_card  # Si tienes columnas de alta cardinalidad\n",
    "                final_feature_names = num_names + ohe_cols + high_card_names\n",
    "    \n",
    "                # Alinear las longitudes de importancia y nombres\n",
    "                min_len = min(len(importance), len(final_feature_names))\n",
    "                importance = importance[:min_len]\n",
    "                final_feature_names = final_feature_names[:min_len]\n",
    "                fi_df = pd.DataFrame({'feature': final_feature_names, 'importance': importance})\n",
    "            else:\n",
    "                # Si no hay preprocesador, usar los nombres originales (Column_0, Column_1, etc.)\n",
    "                fi_df = pd.DataFrame({'feature': feature_names, 'importance': importance})\n",
    "        else:\n",
    "            fi_df = pd.DataFrame({'feature': feature_names, 'importance': importance})\n",
    "    \n",
    "        # Ordenar las importancias y mostrar las top 15\n",
    "        fi_df = fi_df.sort_values(by='importance', ascending=False)\n",
    "    \n",
    "        print(\"\\nTop 15 caracter√≠sticas m√°s importantes:\")\n",
    "        display(fi_df.head(15))\n",
    "    \n",
    "        # Gr√°fico de importancia de caracter√≠sticas\n",
    "        plt.figure(figsize=(10,6))\n",
    "        sns.barplot(data=fi_df.head(20), x='importance', y='feature', palette='viridis')\n",
    "        plt.title(\"Importancia de Caracter√≠sticas (LightGBM)\")\n",
    "        plt.xlabel(\"Ganancia acumulada\")\n",
    "        plt.ylabel(\"Feature\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(\"\\nNo se pudo extraer la importancia de caracter√≠sticas.\")\n",
    "        print(\"Error:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5218d2bf-403d-4519-a641-5d9468e0949a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "    # SHAP + Proxy Scoring\n",
    "    # 1) Extraer modelo y preprocesador\n",
    "    model_lgb = best_model.named_steps['model']\n",
    "    preprocessor = best_model.named_steps['preproc']\n",
    "    \n",
    "    # 2) Transformar X_test usando el preprocesador exactamente como se entren√≥\n",
    "    #    OJO: solo las columnas de entrada originales (sin duplicar te_cols)\n",
    "    X_test_input = X_test[numeric_features + categorical_low_card + te_cols]\n",
    "    X_test_transformed = preprocessor.transform(X_test_input)\n",
    "    \n",
    "    # Reconstruir nombres de features finales tras preprocesamiento\n",
    "    ohe_cols = list(preprocessor.named_transformers_['catlow']\n",
    "                    .named_steps['ohe']\n",
    "                    .get_feature_names_out(categorical_low_card))\n",
    "    final_feature_names = numeric_features + ohe_cols + te_cols\n",
    "    \n",
    "    # 3) Crear TreeExplainer\n",
    "    explainer = shap.TreeExplainer(model_lgb)\n",
    "    shap_values = explainer.shap_values(X_test_transformed)\n",
    "    \n",
    "    # 4) Summary plot\n",
    "    shap.summary_plot(\n",
    "        shap_values,\n",
    "        pd.DataFrame(X_test_transformed, columns=final_feature_names),\n",
    "        max_display=25\n",
    "    )\n",
    "    \n",
    "    # 5) Analizar interacciones\n",
    "    shap_interact = explainer.shap_interaction_values(X_test_transformed)\n",
    "    mean_interact = np.abs(shap_interact).mean(axis=0).sum(axis=1)\n",
    "    interact_df = pd.DataFrame({\n",
    "        'feature': final_feature_names,\n",
    "        'mean_interaction': mean_interact\n",
    "    }).sort_values('mean_interaction', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop features por interacci√≥n media:\")\n",
    "    display(interact_df.head(20))\n",
    "    \n",
    "    # Ejemplo de dependencias:\n",
    "    # shap.dependence_plot('te_raza', shap_values, pd.DataFrame(X_test_transformed, columns=final_feature_names))\n",
    "    # shap.dependence_plot('te_color', shap_values, pd.DataFrame(X_test_transformed, columns=final_feature_names))\n",
    "    \n",
    "    # 6) Proxy Scoring Function\n",
    "    def feature_proxy_score(df_full, feature_name, target_col='precio_venta', time_col='fecha_listado', n_splits=3):\n",
    "        \"\"\"\n",
    "        Calcula un proxy-score combinando:\n",
    "          - correlaci√≥n Spearman con el target\n",
    "          - importancia SHAP (aprox. v√≠a permutation importance)\n",
    "          - estabilidad temporal\n",
    "        \"\"\"\n",
    "        # 1) Correlaci√≥n Spearman\n",
    "        if feature_name in df_full.columns and pd.api.types.is_numeric_dtype(df_full[feature_name]):\n",
    "            corr = abs(df_full[[feature_name, target_col]].dropna().corr(method='spearman').iloc[0, 1])\n",
    "        else:\n",
    "            corr = 0.0\n",
    "    \n",
    "        # 2) SHAP/proxy importance v√≠a permutation importance\n",
    "        try:\n",
    "            perm = permutation_importance(\n",
    "                best_model,\n",
    "                X_test[numeric_features + categorical_low_card],\n",
    "                y_test,\n",
    "                n_repeats=5,\n",
    "                random_state=RND,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "            idx = final_feature_names.index(feature_name) if feature_name in final_feature_names else None\n",
    "            shap_imp = perm.importances_mean[idx] if idx is not None else 0.0\n",
    "        except Exception:\n",
    "            shap_imp = 0.0\n",
    "    \n",
    "        # 3) Estabilidad temporal\n",
    "        stab_change = 0.0\n",
    "        if time_col in df_full.columns:\n",
    "            df_full = df_full.dropna(subset=[time_col])\n",
    "            df_full['period'] = pd.qcut(\n",
    "                df_full[time_col].view('int64') // 10**9,  # convierte a segundos desde epoch\n",
    "                q=n_splits, \n",
    "                duplicates='drop'\n",
    "            )\n",
    "    \n",
    "            imps = []\n",
    "            for p in df_full['period'].unique():\n",
    "                sub = df_full[df_full['period'] == p]\n",
    "                if len(sub) < 50:\n",
    "                    continue\n",
    "                try:\n",
    "                    Xsub = sub[numeric_features + categorical_low_card]\n",
    "                    imp_sub = permutation_importance(best_model, Xsub, sub['precio_venta'], n_repeats=3, random_state=RND, n_jobs=1)\n",
    "                    idx = final_feature_names.index(feature_name) if feature_name in final_feature_names else None\n",
    "                    if idx is not None:\n",
    "                        imps.append(imp_sub.importances_mean[idx])\n",
    "                except Exception:\n",
    "                    pass\n",
    "            if len(imps) >= 2:\n",
    "                stab_change = float(np.std(imps))\n",
    "    \n",
    "        # 4) Score combinado\n",
    "        score = 0.5 * shap_imp + 0.3 * corr - 0.2 * stab_change\n",
    "        return {'feature': feature_name, 'corr': corr, 'shap_imp': shap_imp, 'stab': stab_change, 'proxy_score': score}\n",
    "    \n",
    "    # 7) Calcular proxy scores\n",
    "    proxy_scores = [feature_proxy_score(df, f) for f in final_feature_names if f.startswith('te_')]\n",
    "    proxy_scores_df = pd.DataFrame(proxy_scores).sort_values('proxy_score', ascending=False)\n",
    "    \n",
    "    print(\"\\nProxy scores (top):\")\n",
    "    display(proxy_scores_df.head(20))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
